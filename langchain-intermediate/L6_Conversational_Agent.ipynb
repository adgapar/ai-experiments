{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a3f6a4-8348-4cd6-a336-10ecb9cfaaa3",
   "metadata": {},
   "source": [
    "# Conversational agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36261434-a96c-49e8-ad41-918d14da089a",
   "metadata": {
    "height": 130,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f44fbb-6708-4fce-bac4-55dc261e3a2d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7406eb-334b-43d0-8110-31656a55b2fd",
   "metadata": {
    "height": 691
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "import datetime\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.utcnow()\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}Â°C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac0a1e7-567f-4775-a538-351a5ff099e8",
   "metadata": {
    "height": 351
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4856d233-0916-4b97-8a7c-b494d5f2b8e0",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "tools = [get_current_temperature, search_wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf0d789-559b-42df-9d6d-77a0cd8888d6",
   "metadata": {
    "height": 96
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933ce7cb-099c-4f43-a548-b355a5b66511",
   "metadata": {
    "height": 147
   },
   "outputs": [],
   "source": [
    "functions = [convert_to_openai_function(f) for f in tools]\n",
    "model = ChatOpenAI(temperature=0, model='gpt-4o-mini').bind(functions=functions)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4281a2d8-7884-40e0-b34d-09c39eecbb7a",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='get_current_temperature', tool_input={'latitude': 37.7749, 'longitude': -122.4194}, log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 108, 'total_tokens': 134}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'function_call', 'logprobs': None}, id='run-81c81bce-065d-4bb2-9f19-5ee37d78b91b-0', usage_metadata={'input_tokens': 108, 'output_tokens': 26, 'total_tokens': 134})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"what is the weather is sf?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1dbc697-3e0a-4197-8ddc-2894763b4b85",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab829b59-803f-488f-bcf3-5119c224baa0",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 37.7749, 'longitude': -122.4194}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52b4d483-c498-4932-a373-bc2f4da17c7f",
   "metadata": {
    "height": 130
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ac6ea2-0e20-424a-ba33-6f8d627379b5",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bada1de8-d7c4-4880-98c2-d47cc65f684d",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "result1 = chain.invoke({\n",
    "    \"input\": \"what is the weather is sf?\",\n",
    "    \"agent_scratchpad\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d5f4e61-36a3-4d80-9c49-0b83cedb1592",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11845c11-7d41-455a-8ac0-4b8161acaf83",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/mbtr1z_97_x349r8bdfs7l100000gn/T/ipykernel_63824/3697834985.py:32: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_utc_time = datetime.datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "observation = get_current_temperature.invoke(result1.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "810184c8-8495-4189-b83e-17220cf65bbe",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 22.2Â°C'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1b223d-ea2a-414c-924c-512c4c9da66a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.agents.AgentActionMessageLog"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61853679-75b5-4462-8059-f1214286a9eb",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72ca4278-165a-4915-8ec6-09e696f29d86",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 108, 'total_tokens': 134}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'function_call', 'logprobs': None}, id='run-094b6177-04bd-48bc-b413-bf867b4ae9b7-0', usage_metadata={'input_tokens': 108, 'output_tokens': 26, 'total_tokens': 134})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.message_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27e079a7-d58c-41c0-8762-4e183e72a1d1",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 108, 'total_tokens': 134}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'function_call', 'logprobs': None}, id='run-094b6177-04bd-48bc-b413-bf867b4ae9b7-0', usage_metadata={'input_tokens': 108, 'output_tokens': 26, 'total_tokens': 134}),\n",
       " FunctionMessage(content='The current temperature is 22.2Â°C', name='get_current_temperature')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_to_openai_functions([(result1, observation), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09735a9e-452d-472b-a4c0-35c315e9e53f",
   "metadata": {
    "height": 96
   },
   "outputs": [],
   "source": [
    "result2 = chain.invoke({\n",
    "    \"input\": \"what is the weather is sf?\", \n",
    "    \"agent_scratchpad\": format_to_openai_functions([(result1, observation)])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9df61ecc-52fe-4fae-85a2-dbb61d68e2db",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The weather in San Francisco is a lovely 22.2Â°C. Not too shabby, right? Perfect for a stroll or maybe a little outdoor brunch! ðŸŒž'}, log='The weather in San Francisco is a lovely 22.2Â°C. Not too shabby, right? Perfect for a stroll or maybe a little outdoor brunch! ðŸŒž')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9dd139f-e06a-4148-a025-ae100851c63e",
   "metadata": {
    "height": 300
   },
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = chain.invoke({\n",
    "            \"input\": user_input, \n",
    "            \"agent_scratchpad\": format_to_openai_functions(intermediate_steps)\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f6c1121-70a6-41ae-8c3d-41f7cfc756af",
   "metadata": {
    "height": 96
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a05df88-4ad9-4c48-a7dd-6b42db91cb33",
   "metadata": {
    "height": 283
   },
   "outputs": [],
   "source": [
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke({\n",
    "            \"input\": user_input, \n",
    "            \"intermediate_steps\": intermediate_steps\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8bf88b6-9992-4041-8344-ef4921bf3cf2",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/mbtr1z_97_x349r8bdfs7l100000gn/T/ipykernel_63824/3697834985.py:32: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_utc_time = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The weather in San Francisco is a lovely 22.2Â°C. Not too shabby, right? Perfect for a stroll or maybe a little outdoor brunch! ðŸŒž'}, log='The weather in San Francisco is a lovely 22.2Â°C. Not too shabby, right? Perfect for a stroll or maybe a little outdoor brunch! ðŸŒž')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is the weather is sf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c94a493-b480-409a-968a-038c8895709c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': \"LangChain is a framework that makes it easier to create applications using large language models (LLMs). Think of it as a toolkit for building things like chatbots, document analysis tools, and code analyzers. It was launched in October 2022 by Harrison Chase and quickly gained traction, attracting contributions from hundreds of developers and even securing over $20 million in funding by April 2023.\\n\\nIn 2023, they introduced the LangChain Expression Language (LCEL) for defining action chains and LangServe, a tool for deploying LCEL code as a production-ready API. LangChain supports a wide range of integrations, from cloud storage services to various databases and even web scraping tools. So, if you're into AI and want to whip up some cool applications, LangChain might just be your new best friend!\"}, log=\"LangChain is a framework that makes it easier to create applications using large language models (LLMs). Think of it as a toolkit for building things like chatbots, document analysis tools, and code analyzers. It was launched in October 2022 by Harrison Chase and quickly gained traction, attracting contributions from hundreds of developers and even securing over $20 million in funding by April 2023.\\n\\nIn 2023, they introduced the LangChain Expression Language (LCEL) for defining action chains and LangServe, a tool for deploying LCEL code as a production-ready API. LangChain supports a wide range of integrations, from cloud storage services to various databases and even web scraping tools. So, if you're into AI and want to whip up some cool applications, LangChain might just be your new best friend!\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c8c6fd7-7c8c-487a-81de-1dcff739a2f9",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': \"Hey there! What's up? Need some help or just here to chat?\"}, log=\"Hey there! What's up? Need some help or just here to chat?\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b270a064-8cf1-44da-a59e-be622f9099b9",
   "metadata": {
    "height": 62
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87baebf4-2555-4e98-aece-a1c327644817",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': 'LangChain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "LangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project's Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\n",
      "In the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.\n",
      "In October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.\n",
      "\n",
      "LangChain's developers highlight the framework's applicability to use-cases including chatbots, retrieval-augmented generation,  document summarization, and synthetic data generation.\n",
      "As of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database to store and retrieve vector embeddings; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\n",
      "\n",
      "Official website\n",
      "Discord server support hub\n",
      "Langchain-ai on GitHub\n",
      "\n",
      "Page: Prompt engineering\n",
      "Summary: Prompt engineering is the process of structuring an instruction that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.\n",
      "A prompt for a text-to-text language model can be a query such as \"what is Fermat's little theorem?\", a command such as \"write a poem about leaves falling\", or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as asking the model to complete \"maison â†’ house, chat â†’ cat, chien â†’\" (the expected response being dog), an approach called few-shot learning.\n",
      "When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.\n",
      "\n",
      "Page: DataStax\n",
      "Summary: DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a framework that makes it easier to create applications using large language models (LLMs). Think of it as a toolkit for building things like chatbots, document analysis tools, and code analyzers. It was launched in October 2022 by Harrison Chase and quickly gained traction, thanks to contributions from a ton of developers and a vibrant community.\n",
      "\n",
      "By April 2023, LangChain had become a startup and raised over $20 million in funding. It introduced some cool features like the LangChain Expression Language (LCEL) for defining action chains and LangServe, a tool for deploying code as a production-ready API.\n",
      "\n",
      "LangChain can integrate with various systems, including cloud storage from Amazon, Google, and Microsoft, and it supports a wide range of functionalities, from summarizing documents to generating synthetic data. So, if you're into AI and want to build something nifty, LangChain might just be your new best friend!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is langchain?',\n",
       " 'output': \"LangChain is a framework that makes it easier to create applications using large language models (LLMs). Think of it as a toolkit for building things like chatbots, document analysis tools, and code analyzers. It was launched in October 2022 by Harrison Chase and quickly gained traction, thanks to contributions from a ton of developers and a vibrant community.\\n\\nBy April 2023, LangChain had become a startup and raised over $20 million in funding. It introduced some cool features like the LangChain Expression Language (LCEL) for defining action chains and LangServe, a tool for deploying code as a production-ready API.\\n\\nLangChain can integrate with various systems, including cloud storage from Amazon, Google, and Microsoft, and it supports a wide range of functionalities, from summarizing documents to generating synthetic data. So, if you're into AI and want to build something nifty, LangChain might just be your new best friend!\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2da813c-47c3-487e-8c43-444c9b4d9821",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWell, hello there, Bob! What can I do for you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is bob',\n",
       " 'output': 'Well, hello there, Bob! What can I do for you today?'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"my name is bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15ba8a9f-fcb2-44d0-878f-3017f607e955",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWell, I don't have a crystal ball, so I can't just guess your name. But if you tell me, I promise to remember it for our chat!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name',\n",
       " 'output': \"Well, I don't have a crystal ball, so I can't just guess your name. But if you tell me, I promise to remember it for our chat!\"}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d1df28a-27da-4f8e-ab03-a37a276db74f",
   "metadata": {
    "height": 130
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8598e87-de8a-4710-a90e-2a919adc00e2",
   "metadata": {
    "height": 79
   },
   "outputs": [],
   "source": [
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3865c2c-cb80-4341-bcbe-3b5150749622",
   "metadata": {
    "height": 62
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40b036fc-446f-4075-9a92-9a4bac6ecb2b",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37f70acf-57ff-4ab9-a942-f0ad6e680c40",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWell, hello there, Bob! What can I do for you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is bob',\n",
       " 'chat_history': [HumanMessage(content='my name is bob'),\n",
       "  AIMessage(content='Well, hello there, Bob! What can I do for you today?')],\n",
       " 'output': 'Well, hello there, Bob! What can I do for you today?'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"my name is bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1554b0c-8c72-4d8d-bbed-20afd94828c5",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYour name is Bob! Got any other questions, or are we just playing a game of \"Guess Who\"?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats my name',\n",
       " 'chat_history': [HumanMessage(content='my name is bob'),\n",
       "  AIMessage(content='Well, hello there, Bob! What can I do for you today?'),\n",
       "  HumanMessage(content='whats my name'),\n",
       "  AIMessage(content='Your name is Bob! Got any other questions, or are we just playing a game of \"Guess Who\"?')],\n",
       " 'output': 'Your name is Bob! Got any other questions, or are we just playing a game of \"Guess Who\"?'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40887f61-6746-4e51-852d-5ff0457384b7",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe current temperature is 22.2Â°C\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/mbtr1z_97_x349r8bdfs7l100000gn/T/ipykernel_63824/3697834985.py:32: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_utc_time = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe weather in San Francisco is a lovely 22.2Â°C. Perfect for a stroll or, you know, just pretending to enjoy the outdoors! Need anything else?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats the weather in sf?',\n",
       " 'chat_history': [HumanMessage(content='my name is bob'),\n",
       "  AIMessage(content='Well, hello there, Bob! What can I do for you today?'),\n",
       "  HumanMessage(content='whats my name'),\n",
       "  AIMessage(content='Your name is Bob! Got any other questions, or are we just playing a game of \"Guess Who\"?'),\n",
       "  HumanMessage(content='whats the weather in sf?'),\n",
       "  AIMessage(content='The weather in San Francisco is a lovely 22.2Â°C. Perfect for a stroll or, you know, just pretending to enjoy the outdoors! Need anything else?')],\n",
       " 'output': 'The weather in San Francisco is a lovely 22.2Â°C. Perfect for a stroll or, you know, just pretending to enjoy the outdoors! Need anything else?'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats the weather in sf?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0b511-21d6-4c7e-b4e2-f886bc296997",
   "metadata": {},
   "source": [
    "### Create a chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1cd4bd2-703e-4e4e-8f5c-ae3d491a41f6",
   "metadata": {
    "height": 113
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_your_own(query: str) -> str:\n",
    "    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n",
    "    print(type(query))\n",
    "    return query[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67cb6fdc-ff55-4a7b-8a39-1871006aa990",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "tools = [get_current_temperature, search_wikipedia, create_your_own]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a73e69-acfb-471e-b54d-8ac586235521",
   "metadata": {
    "height": 708
   },
   "outputs": [],
   "source": [
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    \n",
    "    def __init__(self, tools, **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are helpful but sassy assistant\"),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "        ])\n",
    "        self.chain = RunnablePassthrough.assign(\n",
    "            agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "        ) | self.prompt | self.model | OpenAIFunctionsAgentOutputParser()\n",
    "        self.qa = AgentExecutor(agent=self.chain, tools=tools, verbose=False, memory=self.memory)\n",
    "    \n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return\n",
    "        inp.value = ''\n",
    "        result = self.qa.invoke({\"input\": query})\n",
    "        self.answer = result['output'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=450)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=450, styles={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        return pn.WidgetBox(*self.panels, scroll=True)\n",
    "\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c3827-87fc-4dbe-a566-ca95b3f9068e",
   "metadata": {
    "height": 334
   },
   "outputs": [],
   "source": [
    "cb = cbfs(tools)\n",
    "\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text hereâ€¦')\n",
    "\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=400),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# QnA_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1))\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41804e2f-4dc1-4763-8893-5e76ab5d8f14",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fd8bc-9da5-4ca0-bfc9-c1bcf579fa05",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b8125-6f4c-47e9-8654-44388b08bd85",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e1d6f9-fdc9-42eb-84be-dba81bbb63fd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f302e-fcd3-424b-917a-e0e57602b93f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a43ab-06dc-4b48-b279-72230ab855f8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ea19e-5454-4abe-9b6f-d50c0d594151",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef90005-522a-4497-8247-e46fe0e33b3a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffd8a1-3611-4b98-ad00-8abfdc9400bc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f830c57-669f-4408-bc3d-1b7e309ca414",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2004f20-b480-465d-b881-1776963750fc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfab02a-9c9e-4e9c-adb7-b826c76adc5d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbc6dc-e1da-486f-85d9-17d1ca30edef",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9aed9-9202-4a0c-96fb-15353d4ba84f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e8d1e-359f-484c-8ea0-b53c19d689ec",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c54949-26a0-46f9-b9ab-773ea77c4574",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354cfca3-0004-4667-9db4-c1680a32be4d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6178195-8834-48ae-b41c-c1d893027161",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd214a68-f432-4106-aeda-289082ae049e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a4e83-7792-4e4a-8fff-c30dbcaa8be1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b3eed-982f-4e84-8d8d-8c2a7447a717",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337263d-7b23-4584-a9dd-17d89e20e0c3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba85f9-3741-4f37-ba74-28cc59b6acea",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a332c84-8019-4211-a61b-f30f4e35d1f7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1622f-156f-4574-909d-0a276e3afe23",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b279336-c369-4b4b-8d0e-dac95590181f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b5c36-fc6b-4f60-b8da-935873302630",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173aaa1-ede3-4c38-ac46-93b9a7352850",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6869c6-ef60-4274-ad18-a8b2b112c205",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI experiments",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
